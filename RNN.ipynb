{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "RNN.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOL3L49fAXGhOu9/e+2MWGZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MM1026-DS/Deep-learning-logistic-Regression-/blob/master/RNN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKGS_Dp6DSb6"
      },
      "source": [
        "import numpy as np"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlw6t60HDbgL"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "def softmax(x):\n",
        "    e_x = np.exp(x - np.max(x))\n",
        "    return e_x / e_x.sum(axis=0)\n",
        "\n",
        "\n",
        "def sigmoid(x):\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "\n",
        "def initialize_adam(parameters) :\n",
        "    \"\"\"\n",
        "    Initializes v and s as two python dictionaries with:\n",
        "                - keys: \"dW1\", \"db1\", ..., \"dWL\", \"dbL\" \n",
        "                - values: numpy arrays of zeros of the same shape as the corresponding gradients/parameters.\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters.\n",
        "                    parameters[\"W\" + str(l)] = Wl\n",
        "                    parameters[\"b\" + str(l)] = bl\n",
        "    \n",
        "    Returns: \n",
        "    v -- python dictionary that will contain the exponentially weighted average of the gradient.\n",
        "                    v[\"dW\" + str(l)] = ...\n",
        "                    v[\"db\" + str(l)] = ...\n",
        "    s -- python dictionary that will contain the exponentially weighted average of the squared gradient.\n",
        "                    s[\"dW\" + str(l)] = ...\n",
        "                    s[\"db\" + str(l)] = ...\n",
        "\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2 # number of layers in the neural networks\n",
        "    v = {}\n",
        "    s = {}\n",
        "    \n",
        "    # Initialize v, s. Input: \"parameters\". Outputs: \"v, s\".\n",
        "    for l in range(L):\n",
        "    ### START CODE HERE ### (approx. 4 lines)\n",
        "        v[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        v[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "        s[\"dW\" + str(l+1)] = np.zeros(parameters[\"W\" + str(l+1)].shape)\n",
        "        s[\"db\" + str(l+1)] = np.zeros(parameters[\"b\" + str(l+1)].shape)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    return v, s\n",
        "\n",
        "\n",
        "def update_parameters_with_adam(parameters, grads, v, s, t, learning_rate = 0.01,\n",
        "                                beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8):\n",
        "    \"\"\"\n",
        "    Update parameters using Adam\n",
        "    \n",
        "    Arguments:\n",
        "    parameters -- python dictionary containing your parameters:\n",
        "                    parameters['W' + str(l)] = Wl\n",
        "                    parameters['b' + str(l)] = bl\n",
        "    grads -- python dictionary containing your gradients for each parameters:\n",
        "                    grads['dW' + str(l)] = dWl\n",
        "                    grads['db' + str(l)] = dbl\n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    learning_rate -- the learning rate, scalar.\n",
        "    beta1 -- Exponential decay hyperparameter for the first moment estimates \n",
        "    beta2 -- Exponential decay hyperparameter for the second moment estimates \n",
        "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
        "\n",
        "    Returns:\n",
        "    parameters -- python dictionary containing your updated parameters \n",
        "    v -- Adam variable, moving average of the first gradient, python dictionary\n",
        "    s -- Adam variable, moving average of the squared gradient, python dictionary\n",
        "    \"\"\"\n",
        "    \n",
        "    L = len(parameters) // 2                 # number of layers in the neural networks\n",
        "    v_corrected = {}                         # Initializing first moment estimate, python dictionary\n",
        "    s_corrected = {}                         # Initializing second moment estimate, python dictionary\n",
        "    \n",
        "    # Perform Adam update on all parameters\n",
        "    for l in range(L):\n",
        "        # Moving average of the gradients. Inputs: \"v, grads, beta1\". Output: \"v\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v[\"dW\" + str(l+1)] = beta1 * v[\"dW\" + str(l+1)] + (1 - beta1) * grads[\"dW\" + str(l+1)] \n",
        "        v[\"db\" + str(l+1)] = beta1 * v[\"db\" + str(l+1)] + (1 - beta1) * grads[\"db\" + str(l+1)] \n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected first moment estimate. Inputs: \"v, beta1, t\". Output: \"v_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        v_corrected[\"dW\" + str(l+1)] = v[\"dW\" + str(l+1)] / (1 - beta1**t)\n",
        "        v_corrected[\"db\" + str(l+1)] = v[\"db\" + str(l+1)] / (1 - beta1**t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Moving average of the squared gradients. Inputs: \"s, grads, beta2\". Output: \"s\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s[\"dW\" + str(l+1)] = beta2 * s[\"dW\" + str(l+1)] + (1 - beta2) * (grads[\"dW\" + str(l+1)] ** 2)\n",
        "        s[\"db\" + str(l+1)] = beta2 * s[\"db\" + str(l+1)] + (1 - beta2) * (grads[\"db\" + str(l+1)] ** 2)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Compute bias-corrected second raw moment estimate. Inputs: \"s, beta2, t\". Output: \"s_corrected\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        s_corrected[\"dW\" + str(l+1)] = s[\"dW\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        s_corrected[\"db\" + str(l+1)] = s[\"db\" + str(l+1)] / (1 - beta2 ** t)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "        # Update parameters. Inputs: \"parameters, learning_rate, v_corrected, s_corrected, epsilon\". Output: \"parameters\".\n",
        "        ### START CODE HERE ### (approx. 2 lines)\n",
        "        parameters[\"W\" + str(l+1)] = parameters[\"W\" + str(l+1)] - learning_rate * v_corrected[\"dW\" + str(l+1)] / np.sqrt(s_corrected[\"dW\" + str(l+1)] + epsilon)\n",
        "        parameters[\"b\" + str(l+1)] = parameters[\"b\" + str(l+1)] - learning_rate * v_corrected[\"db\" + str(l+1)] / np.sqrt(s_corrected[\"db\" + str(l+1)] + epsilon)\n",
        "        ### END CODE HERE ###\n",
        "\n",
        "    return parameters, v, s"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Ykc_ZD2DiRR"
      },
      "source": [
        "# GRADED FUNCTION: rnn_cell_forward\n",
        "\n",
        "def rnn_cell_forward(xt, a_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implements a single forward step of the RNN-cell as described in Figure (2)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias, numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, a_prev, xt, parameters)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    ### START CODE HERE ### (â‰ˆ2 lines)\n",
        "    # compute next activation state using the formula given above\n",
        "    a_next = np.tanh((np.dot(Waa,a_prev)+np.dot(Wax,xt))+ba)\n",
        "    # compute output of the current cell using the formula given above\n",
        "    yt_pred = softmax(np.dot(Wya,a_next) + by)\n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values you need for backward propagation in cache\n",
        "    cache = (a_next, a_prev, xt, parameters)\n",
        "    \n",
        "    return a_next, yt_pred, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S1-ou-tFD9do",
        "outputId": "5c986843-98ca-47b8-edc1-fd2631203ecc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, yt_pred_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
        "print(\"a_next.shape = \\n\", a_next_tmp.shape)\n",
        "print(\"yt_pred[1] =\\n\", yt_pred_tmp[1])\n",
        "print(\"yt_pred.shape = \\n\", yt_pred_tmp.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] = \n",
            " [ 0.59584544  0.18141802  0.61311866  0.99808218  0.85016201  0.99980978\n",
            " -0.18887155  0.99815551  0.6531151   0.82872037]\n",
            "a_next.shape = \n",
            " (5, 10)\n",
            "yt_pred[1] =\n",
            " [0.9888161  0.01682021 0.21140899 0.36817467 0.98988387 0.88945212\n",
            " 0.36920224 0.9966312  0.9982559  0.17746526]\n",
            "yt_pred.shape = \n",
            " (2, 10)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "COZYHxbkEYFt"
      },
      "source": [
        "# GRADED FUNCTION: rnn_forward\n",
        "\n",
        "def rnn_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network described in Figure (3).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Waa -- Weight matrix multiplying the hidden state, numpy array of shape (n_a, n_a)\n",
        "                        Wax -- Weight matrix multiplying the input, numpy array of shape (n_a, n_x)\n",
        "                        Wya -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        ba --  Bias numpy array of shape (n_a, 1)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "\n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y_pred -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of caches, x)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Initialize \"caches\" which will contain the list of all caches\n",
        "    caches = []\n",
        "    \n",
        "    # Retrieve dimensions from shapes of x and parameters[\"Wya\"]\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = parameters[\"Wya\"].shape\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    \n",
        "    # initialize \"a\" and \"y_pred\" with zeros (â‰ˆ2 lines)\n",
        "    a = np.zeros(shape = (n_a,m,T_x))\n",
        "    y_pred = np.zeros(shape = (n_y,m,T_x))\n",
        "    \n",
        "    # Initialize a_next (â‰ˆ1 line)\n",
        "    a_next = a0\n",
        "    \n",
        "    # loop over all time-steps of the input 'x' (1 line)\n",
        "    for t in range(T_x):\n",
        "        # Update next hidden state, compute the prediction, get the cache (â‰ˆ2 lines)\n",
        "        xt = rnn_cell_forward(x[:,:,t], a_next, parameters)\n",
        "        a_next, yt_pred, cache = xt\n",
        "        # Save the value of the new \"next\" hidden state in a (â‰ˆ1 line)\n",
        "        a[:,:,t] = a_next\n",
        "        # Save the value of the prediction in y (â‰ˆ1 line)\n",
        "        y_pred[:,:,t] = yt_pred\n",
        "        # Append \"cache\" to \"caches\" (â‰ˆ1 line)\n",
        "        caches.append(cache)\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "    \n",
        "    return a, y_pred, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WIknT0kiG2hq",
        "outputId": "596b37a6-2718-4748-9cf1-209e188c17c3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,4)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_pred_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "print(\"a[4][1] = \\n\", a_tmp[4][1])\n",
        "print(\"a.shape = \\n\", a_tmp.shape)\n",
        "print(\"y_pred[1][3] =\\n\", y_pred_tmp[1][3])\n",
        "print(\"y_pred.shape = \\n\", y_pred_tmp.shape)\n",
        "print(\"caches[1][1][3] =\\n\", caches_tmp[1][1][3])\n",
        "print(\"len(caches) = \\n\", len(caches_tmp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][1] = \n",
            " [-0.99999375  0.77911235 -0.99861469 -0.99833267]\n",
            "a.shape = \n",
            " (5, 10, 4)\n",
            "y_pred[1][3] =\n",
            " [0.79560373 0.86224861 0.11118257 0.81515947]\n",
            "y_pred.shape = \n",
            " (2, 10, 4)\n",
            "caches[1][1][3] =\n",
            " [-1.1425182  -0.34934272 -0.20889423  0.58662319]\n",
            "len(caches) = \n",
            " 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O9qVajRALKA6"
      },
      "source": [
        "## LSTM (LONG - SHORT TERM MEMORY)\n",
        "Long short term is more powerful in compariso of simple RNN.\n",
        "Why LSTM ?\n",
        "eg:the cat was eating so much...............was not able to move.\n",
        "in above exapmple verb used with cat is singular i.e was so for long sentence there is chhance is that the simple RNN will forget(Vanishing gradient ) that it should be singular nonu i.e should be was\n",
        "\n",
        "What is Vanishing or Exploding gradient ?\n",
        "if you are working on deep neural network of many hidden layer there is chance \n",
        "of vanishing the gradient that means eg:\n",
        "y=w1w2---------------wlz\n",
        "we are taking the linear network\n",
        "what will happen that\n",
        "w[.5 0\n",
        "  0  .5]^l\n",
        "if I is greater than 1 it become exploding gradient descent and smaller than 1 it become vanishing gradient descent \n",
        "________________________________________________________\n",
        " <ul>\n",
        " <li>C~t = np.tanh(np.dot(Wca,a_previous)+np.dot(Wcx,xt)+bc)--It is working as a memory function</li>\n",
        "\n",
        "(In LSTM  there is 3 gates)\n",
        "<li>sigma_updated= sigmoid(np.dot(Wua,a_previous)+np.dot(Wux,xt)+bu)(updated gate)</li>\n",
        "<li>sigma_forget(l`) =  sigmoid(np.dot(Wfa,a_previous) + np.dot(Wfx,xt)+bf)</li>\n",
        "<li>sigma_output = sigmoid(np.dot(Woa,a_previous)+np.dot(Wux,xt)+bo)</li>\n",
        "<li> c<t> = sigma_update*C~t+sigma_forget*c<t-1> </li>\n",
        "<li> a<t> = sigma_output*np.tanh(c<t>)</li>\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2QhMhwp0HxEC"
      },
      "source": [
        "# GRADED FUNCTION: lstm_cell_forward\n",
        "\n",
        "def lstm_cell_forward(xt, a_prev, c_prev, parameters):\n",
        "    \"\"\"\n",
        "    Implement a single forward step of the LSTM-cell as described in Figure (4)\n",
        "\n",
        "    Arguments:\n",
        "    xt -- your input data at timestep \"t\", numpy array of shape (n_x, m).\n",
        "    a_prev -- Hidden state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    c_prev -- Memory state at timestep \"t-1\", numpy array of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc --  Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo --  Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a_next -- next hidden state, of shape (n_a, m)\n",
        "    c_next -- next memory state, of shape (n_a, m)\n",
        "    yt_pred -- prediction at timestep \"t\", numpy array of shape (n_y, m)\n",
        "    cache -- tuple of values needed for the backward pass, contains (a_next, c_next, a_prev, c_prev, xt, parameters)\n",
        "    \n",
        "    Note: ft/it/ot stand for the forget/update/output gates, cct stands for the candidate value (c tilde),\n",
        "          c stands for the cell state (memory)\n",
        "    \"\"\"\n",
        "\n",
        "    # Retrieve parameters from \"parameters\"\n",
        "    Wf = parameters[\"Wf\"] # forget gate weight\n",
        "    bf = parameters[\"bf\"]\n",
        "    Wi = parameters[\"Wi\"] # update gate weight (notice the variable name)\n",
        "    bi = parameters[\"bi\"] # (notice the variable name)\n",
        "    Wc = parameters[\"Wc\"] # candidate value weight\n",
        "    bc = parameters[\"bc\"]\n",
        "    Wo = parameters[\"Wo\"] # output gate weight\n",
        "    bo = parameters[\"bo\"]\n",
        "    Wy = parameters[\"Wy\"] # prediction weight\n",
        "    by = parameters[\"by\"]\n",
        "    \n",
        "    # Retrieve dimensions from shapes of xt and Wy\n",
        "    n_x, m = xt.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # Concatenate a_prev and xt (â‰ˆ1 line)\n",
        "    concat = np.concatenate((a_prev,xt))\n",
        "\n",
        "    # Compute values for ft (forget gate), it (update gate),\n",
        "    # cct (candidate value), c_next (cell state), \n",
        "    # ot (output gate), a_next (hidden state) (â‰ˆ6 lines)\n",
        "    ft = sigmoid(np.dot(Wf,concat)+bf)        # forget gate\n",
        "    it = sigmoid(np.dot(Wi,concat)+bi)        # update gate\n",
        "    cct = np.tanh(np.dot(Wc,concat)+bc)       # candidate value\n",
        "    # c_next = np.dot(it,cct)+np.dot(ft,c_prev)    # cell state\n",
        "    c_next = it*cct  + ft*c_prev\n",
        "    ot = sigmoid(np.dot(Wo,concat)+bo)        # output gate\n",
        "    a_next = ot*np.tanh(c_next)   # hidden state\n",
        "    \n",
        "    # Compute prediction of the LSTM cell (â‰ˆ1 line)\n",
        "    yt_pred = softmax((np.dot(Wy,a_next)+by))\n",
        "    ### END CODE HERE ###\n",
        "\n",
        "    # store values needed for backward propagation in cache\n",
        "    cache = (a_next, c_next, a_prev, c_prev, ft, it, cct, ot, xt, parameters)\n",
        "\n",
        "    return a_next, c_next, yt_pred, cache"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSqX8syWVhVh",
        "outputId": "d53c811e-0ba8-4540-8605-994bccd33202",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "c_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bf'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bi'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bo'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bc'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, c_next_tmp, yt_tmp, cache_tmp = lstm_cell_forward(xt_tmp, a_prev_tmp, c_prev_tmp, parameters_tmp)\n",
        "print(\"a_next[4] = \\n\", a_next_tmp[4])\n",
        "print(\"a_next.shape = \", a_next_tmp.shape)\n",
        "print(\"c_next[2] = \\n\", c_next_tmp[2])\n",
        "print(\"c_next.shape = \", c_next_tmp.shape)\n",
        "print(\"yt[1] =\", yt_tmp[1])\n",
        "print(\"yt.shape = \", yt_tmp.shape)\n",
        "print(\"cache[1][3] =\\n\", cache_tmp[1][3])\n",
        "print(\"len(cache) = \", len(cache_tmp))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a_next[4] = \n",
            " [-0.66408471  0.0036921   0.02088357  0.22834167 -0.85575339  0.00138482\n",
            "  0.76566531  0.34631421 -0.00215674  0.43827275]\n",
            "a_next.shape =  (5, 10)\n",
            "c_next[2] = \n",
            " [ 0.63267805  1.00570849  0.35504474  0.20690913 -1.64566718  0.11832942\n",
            "  0.76449811 -0.0981561  -0.74348425 -0.26810932]\n",
            "c_next.shape =  (5, 10)\n",
            "yt[1] = [0.79913913 0.15986619 0.22412122 0.15606108 0.97057211 0.31146381\n",
            " 0.00943007 0.12666353 0.39380172 0.07828381]\n",
            "yt.shape =  (2, 10)\n",
            "cache[1][3] =\n",
            " [-0.16263996  1.03729328  0.72938082 -0.54101719  0.02752074 -0.30821874\n",
            "  0.07651101 -1.03752894  1.41219977 -0.37647422]\n",
            "len(cache) =  10\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFTq9m8WH5-8"
      },
      "source": [
        "# GRADED FUNCTION: lstm_forward\n",
        "\n",
        "def lstm_forward(x, a0, parameters):\n",
        "    \"\"\"\n",
        "    Implement the forward propagation of the recurrent neural network using an LSTM-cell described in Figure (4).\n",
        "\n",
        "    Arguments:\n",
        "    x -- Input data for every time-step, of shape (n_x, m, T_x).\n",
        "    a0 -- Initial hidden state, of shape (n_a, m)\n",
        "    parameters -- python dictionary containing:\n",
        "                        Wf -- Weight matrix of the forget gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bf -- Bias of the forget gate, numpy array of shape (n_a, 1)\n",
        "                        Wi -- Weight matrix of the update gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bi -- Bias of the update gate, numpy array of shape (n_a, 1)\n",
        "                        Wc -- Weight matrix of the first \"tanh\", numpy array of shape (n_a, n_a + n_x)\n",
        "                        bc -- Bias of the first \"tanh\", numpy array of shape (n_a, 1)\n",
        "                        Wo -- Weight matrix of the output gate, numpy array of shape (n_a, n_a + n_x)\n",
        "                        bo -- Bias of the output gate, numpy array of shape (n_a, 1)\n",
        "                        Wy -- Weight matrix relating the hidden-state to the output, numpy array of shape (n_y, n_a)\n",
        "                        by -- Bias relating the hidden-state to the output, numpy array of shape (n_y, 1)\n",
        "                        \n",
        "    Returns:\n",
        "    a -- Hidden states for every time-step, numpy array of shape (n_a, m, T_x)\n",
        "    y -- Predictions for every time-step, numpy array of shape (n_y, m, T_x)\n",
        "    c -- The value of the cell state, numpy array of shape (n_a, m, T_x)\n",
        "    caches -- tuple of values needed for the backward pass, contains (list of all the caches, x)\n",
        "    \"\"\"\n",
        "\n",
        "    # Initialize \"caches\", which will track the list of all the caches\n",
        "    caches = []\n",
        "    \n",
        "    ### START CODE HERE ###\n",
        "    Wy = parameters['Wy'] # saving parameters['Wy'] in a local variable in case students use Wy instead of parameters['Wy']\n",
        "    # Retrieve dimensions from shapes of x and parameters['Wy'] (â‰ˆ2 lines)\n",
        "    n_x, m, T_x = x.shape\n",
        "    n_y, n_a = Wy.shape\n",
        "    \n",
        "    # initialize \"a\", \"c\" and \"y\" with zeros (â‰ˆ3 lines)\n",
        "    a = np.zeros((n_a,m,T_x))\n",
        "    c = np.zeros((n_a,m,T_x))\n",
        "    y = np.zeros((n_y,m,T_x))\n",
        "    \n",
        "    # Initialize a_next and c_next (â‰ˆ2 lines)\n",
        "    a_next = a0\n",
        "    c_next = np.zeros((n_a,m))\n",
        "    \n",
        "    # loop over all time-steps\n",
        "    for t in range(T_x):\n",
        "        # Get the 2D slice 'xt' from the 3D input 'x' at time step 't'\n",
        "        xt = x[:,:,t ]\n",
        "        # Update next hidden state, next memory state, compute the prediction, get the cache (â‰ˆ1 line)\n",
        "        a_next, c_next, yt, cache = lstm_cell_forward(x[:,:,t], a_next, c_next, parameters)\n",
        "        # Save the value of the new \"next\" hidden state in a (â‰ˆ1 line)\n",
        "        a[:,:,t] = a_next\n",
        "        # Save the value of the next cell state (â‰ˆ1 line)\n",
        "        c[:,:,t]  = c_next\n",
        "        # Save the value of the prediction in y (â‰ˆ1 line)\n",
        "        y[:,:,t] = yt\n",
        "        # Append the cache into caches (â‰ˆ1 line)\n",
        "        caches.append(cache)\n",
        "        \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # store values needed for backward propagation in cache\n",
        "    caches = (caches, x)\n",
        "\n",
        "    return a, y, c, caches"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5s6HYPP7NOH",
        "outputId": "369ee07a-5362-4d6d-974e-bbec223635cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,7)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wf'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bf'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wi'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bi']= np.random.randn(5,1)\n",
        "parameters_tmp['Wo'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bo'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wc'] = np.random.randn(5, 5+3)\n",
        "parameters_tmp['bc'] = np.random.randn(5,1)\n",
        "parameters_tmp['Wy'] = np.random.randn(2,5)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_tmp, c_tmp, caches_tmp = lstm_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "print(\"a[4][3][6] = \", a_tmp[4][3][6])\n",
        "print(\"a.shape = \", a_tmp.shape)\n",
        "print(\"y[1][4][3] =\", y_tmp[1][4][3])\n",
        "print(\"y.shape = \", y_tmp.shape)\n",
        "print(\"caches[1][1][1] =\\n\", caches_tmp[1][1][1])\n",
        "print(\"c[1][2][1]\", c_tmp[1][2][1])\n",
        "print(\"len(caches) = \", len(caches_tmp))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a[4][3][6] =  0.17211776753291672\n",
            "a.shape =  (5, 10, 7)\n",
            "y[1][4][3] = 0.9508734618501101\n",
            "y.shape =  (2, 10, 7)\n",
            "caches[1][1][1] =\n",
            " [ 0.82797464  0.23009474  0.76201118 -0.22232814 -0.20075807  0.18656139\n",
            "  0.41005165]\n",
            "c[1][2][1] -0.8555449167181981\n",
            "len(caches) =  2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hOKdGqP1DjQa"
      },
      "source": [
        "def rnn_cell_backward(da_next, cache):\n",
        "    \"\"\"\n",
        "    Implements the backward pass for the RNN-cell (single time-step).\n",
        "\n",
        "    Arguments:\n",
        "    da_next -- Gradient of loss with respect to next hidden state\n",
        "    cache -- python dictionary containing useful values (output of rnn_cell_forward())\n",
        "\n",
        "    Returns:\n",
        "    gradients -- python dictionary containing:\n",
        "                        dx -- Gradients of input data, of shape (n_x, m)\n",
        "                        da_prev -- Gradients of previous hidden state, of shape (n_a, m)\n",
        "                        dWax -- Gradients of input-to-hidden weights, of shape (n_a, n_x)\n",
        "                        dWaa -- Gradients of hidden-to-hidden weights, of shape (n_a, n_a)\n",
        "                        dba -- Gradients of bias vector, of shape (n_a, 1)\n",
        "    \"\"\"\n",
        "    \n",
        "    # Retrieve values from cache\n",
        "    (a_next, a_prev, xt, parameters) = cache\n",
        "    \n",
        "    # Retrieve values from parameters\n",
        "    Wax = parameters[\"Wax\"]\n",
        "    Waa = parameters[\"Waa\"]\n",
        "    Wya = parameters[\"Wya\"]\n",
        "    ba = parameters[\"ba\"]\n",
        "    by = parameters[\"by\"]\n",
        "\n",
        "    ### START CODE HERE ###\n",
        "    # compute the gradient of the loss with respect to z (optional) (â‰ˆ1 line)\n",
        "    dz = None\n",
        "\n",
        "    # compute the gradient of the loss with respect to Wax (â‰ˆ2 lines)\n",
        "    dxt = np.dot(Wax.T,(da_next*(1-(np.tanh((np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)*(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba))))))\n",
        "    dWax = np.dot((da_next*(1-(np.tanh((np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba))**2))),xt.T)\n",
        "\n",
        "    # compute the gradient with respect to Waa (â‰ˆ2 lines)\n",
        "    da_prev = np.dot(Waa.T ,(da_next*(1-(np.tanh((np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)*(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba))))))\n",
        "    dWaa = np.dot((da_next*(1-(np.tanh((np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba))**2))),a_prev.T)\n",
        "\n",
        "    # compute the gradient with respect to b (â‰ˆ1 line)\n",
        "    dba = np.sum((da_next*(1-(np.tanh((np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba)*(np.dot(Wax,xt)+np.dot(Waa,a_prev)+ba))))),axis=1)\n",
        " \n",
        "    ### END CODE HERE ###\n",
        "    \n",
        "    # Store the gradients in a python dictionary\n",
        "    gradients = {\"dxt\": dxt, \"da_prev\": da_prev, \"dWax\": dWax, \"dWaa\": dWaa, \"dba\": dba}\n",
        "    \n",
        "    return gradients"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPMYVQRZNY-O",
        "outputId": "70b9cdba-2a39-4c02-9034-ae490987bdb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "np.random.seed(1)\n",
        "xt_tmp = np.random.randn(3,10)\n",
        "a_prev_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_next_tmp, yt_tmp, cache_tmp = rnn_cell_forward(xt_tmp, a_prev_tmp, parameters_tmp)\n",
        "\n",
        "da_next_tmp = np.random.randn(5,10)\n",
        "gradients_tmp = rnn_cell_backward(da_next_tmp, cache_tmp)\n",
        "print(\"gradients[\\\"dxt\\\"][1][2] =\", gradients_tmp[\"dxt\"][1][2])\n",
        "print(\"gradients[\\\"dxt\\\"].shape =\", gradients_tmp[\"dxt\"].shape)\n",
        "print(\"gradients[\\\"da_prev\\\"][2][3] =\", gradients_tmp[\"da_prev\"][2][3])\n",
        "print(\"gradients[\\\"da_prev\\\"].shape =\", gradients_tmp[\"da_prev\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "gradients[\"dxt\"][1][2] = -1.0171023380072393\n",
            "gradients[\"dxt\"].shape = (3, 10)\n",
            "gradients[\"da_prev\"][2][3] = -0.15784531210210753\n",
            "gradients[\"da_prev\"].shape = (5, 10)\n",
            "gradients[\"dWax\"][3][1] = 0.4107728249354584\n",
            "gradients[\"dWax\"].shape = (5, 3)\n",
            "gradients[\"dWaa\"][1][2] = 1.1503450668497135\n",
            "gradients[\"dWaa\"].shape = (5, 5)\n",
            "gradients[\"dba\"][4] = 0.003949177176312643\n",
            "gradients[\"dba\"].shape = (5,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ama_EwJGDn6j"
      },
      "source": [
        "radients[\"dxt\"][1][2] =\t-1.3872130506\n",
        "gradients[\"dxt\"].shape =\t(3, 10)\n",
        "gradients[\"da_prev\"][2][3] =\t-0.152399493774\n",
        "gradients[\"da_prev\"].shape =\t(5, 10)\n",
        "gradients[\"dWax\"][3][1] =\t0.410772824935\n",
        "gradients[\"dWax\"].shape =\t(5, 3)\n",
        "gradients[\"dWaa\"][1][2] =\t1.15034506685\n",
        "gradients[\"dWaa\"].shape =\t(5, 5)\n",
        "gradients[\"dba\"][4] =\t[ 0.20023491]\n",
        "gradients[\"dba\"].shape =\t(5, 1)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3zw9vQRXhdb6"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICF7lif4Lz2g"
      },
      "source": [
        "np.random.seed(1)\n",
        "x_tmp = np.random.randn(3,10,4)\n",
        "a0_tmp = np.random.randn(5,10)\n",
        "parameters_tmp = {}\n",
        "parameters_tmp['Wax'] = np.random.randn(5,3)\n",
        "parameters_tmp['Waa'] = np.random.randn(5,5)\n",
        "parameters_tmp['Wya'] = np.random.randn(2,5)\n",
        "parameters_tmp['ba'] = np.random.randn(5,1)\n",
        "parameters_tmp['by'] = np.random.randn(2,1)\n",
        "\n",
        "a_tmp, y_tmp, caches_tmp = rnn_forward(x_tmp, a0_tmp, parameters_tmp)\n",
        "da_tmp = np.random.randn(5, 10, 4)\n",
        "gradients_tmp = rnn_backward(da_tmp, caches_tmp)\n",
        "\n",
        "print(\"gradients[\\\"dx\\\"][1][2] =\", gradients_tmp[\"dx\"][1][2])\n",
        "print(\"gradients[\\\"dx\\\"].shape =\", gradients_tmp[\"dx\"].shape)\n",
        "print(\"gradients[\\\"da0\\\"][2][3] =\", gradients_tmp[\"da0\"][2][3])\n",
        "print(\"gradients[\\\"da0\\\"].shape =\", gradients_tmp[\"da0\"].shape)\n",
        "print(\"gradients[\\\"dWax\\\"][3][1] =\", gradients_tmp[\"dWax\"][3][1])\n",
        "print(\"gradients[\\\"dWax\\\"].shape =\", gradients_tmp[\"dWax\"].shape)\n",
        "print(\"gradients[\\\"dWaa\\\"][1][2] =\", gradients_tmp[\"dWaa\"][1][2])\n",
        "print(\"gradients[\\\"dWaa\\\"].shape =\", gradients_tmp[\"dWaa\"].shape)\n",
        "print(\"gradients[\\\"dba\\\"][4] =\", gradients_tmp[\"dba\"][4])\n",
        "print(\"gradients[\\\"dba\\\"].shape =\", gradients_tmp[\"dba\"].shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ILXASWcTndaS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}